{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy.stats\n",
    "import numpy as np\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data\n",
    "df = pd.read_csv('D:/My stuff/School/Master/Master Thesis/Data/Kaggle/Home credit/application_train.csv', na_values = [\"\", \"NA\", \"XNA\"])\n",
    "var_info = pd.read_excel('D:/My stuff/School/Master/Master Thesis/Data/Kaggle/Home credit/HomeCredit_columns_description.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(307511, 122)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check shape\n",
    "df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop the ID column\n",
    "df.drop('SK_ID_CURR', axis = 1, inplace = True)\n",
    "var_info.drop(0, axis = 0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename columns for clarity\n",
    "df.rename(columns = {'TARGET' : 'Target'}, inplace = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop variables with low fill rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 50 columns with percentage of missings higher than 0.2\n"
     ]
    }
   ],
   "source": [
    "#Drop variables with too many missing values\n",
    "miss_perc = df.isna().sum() / df.shape[0] > 0.2\n",
    "print(f'Dropped {sum(miss_perc)} columns with percentage of missings higher than 0.2')\n",
    "df = df.loc[:, np.invert(miss_perc)]\n",
    "var_info = var_info.loc[np.invert(miss_perc.values), :]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retain only categorical features with two unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 10 categorical features with more or less than 2 unique values\n"
     ]
    }
   ],
   "source": [
    "#Get number of unique values\n",
    "n_unique = df.nunique() #Get number of unique values (missings disregarded)\n",
    "cat_to_drop = (n_unique != 2) & (var_info['Type'] == 'categorical').values\n",
    "print(f'Dropping {sum(cat_to_drop)} categorical features with more or less than 2 unique values')\n",
    "df = df.loc[:, np.invert(cat_to_drop)] #Drop the features\n",
    "var_info = var_info.loc[np.invert(cat_to_drop.values), :]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dummy variables\n",
    "df.rename(columns = {'NAME_CONTRACT_TYPE' : 'CASH_LOANS', 'CODE_GENDER' : 'FEMALE'}, inplace = True) #Rename two columns for clarity\n",
    "var_info['Row'].replace({'NAME_CONTRACT_TYPE' : 'CASH_LOANS', 'CODE_GENDER' : 'FEMALE'}, inplace = True) #Rename in metadata as well\n",
    "df['CASH_LOANS'] = df['CASH_LOANS'].map({'Cash loans' : 1, 'Revolving loans' : 0}, na_action = 'ignore')\n",
    "df['FEMALE'] = df['FEMALE'].map({'F' : 1, 'M' : 0}, na_action = 'ignore')\n",
    "df['FLAG_OWN_CAR'] = df['FLAG_OWN_CAR'].map({'Y' : 1, 'N' : 0}, na_action = 'ignore')\n",
    "df['FLAG_OWN_REALTY'] = df['FLAG_OWN_REALTY'].map({'Y' : 1, 'N' : 0}, na_action = 'ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop correlated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specifications\n",
    "dep_var = 'Target'\n",
    "thres = 0.75 #Arbitrary for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the correlations\n",
    "indep_vars = list(set(df.columns.to_list()) - set([dep_var])) #Specify the list of independent variables\n",
    "corr_mat = df.corr() #Calculate the correlation matrix\n",
    "dep_cor = corr_mat[dep_var].copy() #Extract the correlations with the dependent variable\n",
    "corr_mat.drop(dep_var, axis = 0) #Drop the row with correlations with the dependent variable\n",
    "corr_mat.drop(dep_var, axis = 1) #Drop the column with the correlations with the dependent variable\n",
    "corr_mat.values[np.tril_indices_from(corr_mat.values)] = np.nan #Leave only the upper triangle\n",
    "corr_mat = corr_mat.unstack().dropna().reset_index() #Unstack to a table\n",
    "corr_mat.columns = ['Var 1', 'Var 2', 'Corr'] #Rename columns for clarity\n",
    "corr_mat = corr_mat.loc[np.argsort(-corr_mat['Corr'].abs(), ), :].reset_index(drop = True) #Sort in absolute values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable FLAG_EMP_PHONE dropped due to correlation of -99.98% with DAYS_EMPLOYED\n",
      "Variable OBS_60_CNT_SOCIAL_CIRCLE dropped due to correlation of 99.85% with OBS_30_CNT_SOCIAL_CIRCLE\n",
      "Variable AMT_CREDIT dropped due to correlation of 98.70% with AMT_GOODS_PRICE\n",
      "Variable CNT_FAM_MEMBERS dropped due to correlation of 87.92% with CNT_CHILDREN\n",
      "Variable LIVE_REGION_NOT_WORK_REGION dropped due to correlation of 86.06% with REG_REGION_NOT_WORK_REGION\n",
      "Variable DEF_60_CNT_SOCIAL_CIRCLE dropped due to correlation of 86.05% with DEF_30_CNT_SOCIAL_CIRCLE\n",
      "Variable LIVE_CITY_NOT_WORK_CITY dropped due to correlation of 82.56% with REG_CITY_NOT_WORK_CITY\n",
      "Variable AMT_ANNUITY dropped due to correlation of 77.51% with AMT_GOODS_PRICE\n",
      "Dropped 8 features due to high correlation\n"
     ]
    }
   ],
   "source": [
    "#Drop correlated features\n",
    "n_dropped = 0 #Initiate the number of dropped features\n",
    "for i in corr_mat.index[corr_mat['Corr'].abs() >= thres]: #Loop through all correlations higher than threshold (in absolute value)\n",
    "    var1, var2 = corr_mat.loc[i, 'Var 1'], corr_mat.loc[i, 'Var 2'] #Store variable names\n",
    "    if (var1 not in indep_vars) | (var2 not in indep_vars):\n",
    "        continue #Skip the iteration if one of the variables has already been disregarded\n",
    "    var_types = var_info.loc[var_info['Row'] == var1, 'Type'].values[0], var_info.loc[var_info['Row'] == var2, 'Type'].values[0] #Store variable types\n",
    "    if var_types.count('categorical') == 1: #If only one of the variables is categorical, retain the numerical one\n",
    "        var_to_drop = [var1, var2][var_types.index('categorical')] #Store the categorical variable to drop\n",
    "        not_dropped_var = [var1, var2][var_types.index('numerical')] #Store the numerical variable to retain\n",
    "    else:\n",
    "        min_id = np.argmin(dep_cor[corr_mat.loc[i, ['Var 1', 'Var 2']].values].abs()) #Find the id of the variable with the smallest absolute correlation with the dependent variable\n",
    "        var_to_drop = corr_mat.loc[i, f'Var {min_id + 1}'] #Get the name of the variable to be dropped\n",
    "        not_dropped_var = corr_mat.loc[i, f'Var {abs(min_id - 2)}'] #Get the name of the variable that was not dropped (for logging purposes)\n",
    "    indep_vars.remove(var_to_drop) #Drop the variable\n",
    "    print(f'Variable {var_to_drop} dropped due to correlation of {corr_mat.loc[i, \"Corr\"]:.2%} with {not_dropped_var}')\n",
    "    n_dropped += 1\n",
    "print(f'Dropped {n_dropped} features due to high correlation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter out dropped variables\n",
    "df = df.loc[:, indep_vars + [dep_var]]\n",
    "var_info = var_info.loc[var_info['Row'].isin(indep_vars + [dep_var]).values, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate VIF\n",
    "#vif_dict = {val:round(variance_inflation_factor(df.loc[:, indep_vars].dropna(), idx), 2) for idx, val in enumerate(indep_vars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check VIF values\n",
    "# for i in vif_dict:\n",
    "#     if (vif_dict[i] > 10) | pd.isna(vif_dict[i]):\n",
    "#         print(i, vif_dict[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop FLAG_DOCUMENT_2 and recalculate VIF\n",
    "indep_vars.remove('FLAG_DOCUMENT_2')\n",
    "# vif_dict = {val:round(variance_inflation_factor(df.loc[:, indep_vars].dropna(), idx), 2) for idx, val in enumerate(indep_vars)}\n",
    "# for i in vif_dict:\n",
    "#     if (vif_dict[i] > 10) | pd.isna(vif_dict[i]):\n",
    "#         print(i, vif_dict[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop FLAG_MOBIL and recalculate VIF\n",
    "indep_vars.remove('FLAG_MOBIL')\n",
    "# vif_dict = {val:round(variance_inflation_factor(df.loc[:, indep_vars].dropna(), idx), 2) for idx, val in enumerate(indep_vars)}\n",
    "# for i in vif_dict:\n",
    "#     if (vif_dict[i] > 10) | pd.isna(vif_dict[i]):\n",
    "#         print(i, vif_dict[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop FLAG_DOCUMENT_3 and recalculate VIF for all\n",
    "indep_vars.remove('FLAG_DOCUMENT_3')\n",
    "# vif_dict = {val:round(variance_inflation_factor(df.loc[:, indep_vars].dropna(), idx), 2) for idx, val in enumerate(indep_vars)}\n",
    "# for i in vif_dict:\n",
    "#     if vif_dict[i] > 10:\n",
    "#         print(i, vif_dict[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop FLAG_CONT_MOBILE and recalculate VIF for all\n",
    "indep_vars.remove('FLAG_CONT_MOBILE')\n",
    "# vif_dict = {val:round(variance_inflation_factor(df.loc[:, indep_vars].dropna(), idx), 2) for idx, val in enumerate(indep_vars)}\n",
    "# for i in vif_dict:\n",
    "#     if vif_dict[i] > 10:\n",
    "#         print(i, vif_dict[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop DAYS_BIRTH and recalculate VIF for all\n",
    "indep_vars.remove('DAYS_BIRTH')\n",
    "# vif_dict = {val:round(variance_inflation_factor(df.loc[:, indep_vars].dropna(), idx), 2) for idx, val in enumerate(indep_vars)}\n",
    "# for i in vif_dict:\n",
    "#     if vif_dict[i] > 10:\n",
    "#         print(i, vif_dict[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop the variables disregarded by VIF\n",
    "df = df.loc[:, indep_vars + [dep_var]]\n",
    "var_info = var_info.loc[var_info['Row'].isin(indep_vars + [dep_var]).values, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(307511, 48)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check shape after filtering\n",
    "df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245155 out of 307511 observations left after removing NAs (62356 observations dropped)\n"
     ]
    }
   ],
   "source": [
    "#Drop NAs\n",
    "df_no_nas = df.dropna()\n",
    "print(f'{df_no_nas.shape[0]} out of {df.shape[0]} observations left after removing NAs ({df.shape[0] - df_no_nas.shape[0]} observations dropped)')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove categorical variables\n",
    "df_no_cats = df_no_nas.loc[:, df_no_nas.nunique() > 2].copy()\n",
    "df_no_cats['Target'] = df_no_nas['Target'] #Retain target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(245155, 19)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check shape\n",
    "df_no_cats.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get info about final state of the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(245155, 48)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check shape\n",
    "df_no_nas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07779160123187372"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get average of the target variable\n",
    "df_no_nas['Target'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check number of categorical variables\n",
    "cat_vars_final_no = sum(df_no_nas.nunique() <= 2) - 1\n",
    "cat_vars_final_no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check number of numerical variables\n",
    "df_no_nas.shape[1] - cat_vars_final_no - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trim outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLAG_WORK_PHONE: 0 observations dropped\n",
      "FLAG_DOCUMENT_14: 778 observations dropped\n",
      "AMT_INCOME_TOTAL: 4862 observations dropped\n",
      "FLAG_DOCUMENT_19: 153 observations dropped\n",
      "FLAG_DOCUMENT_21: 75 observations dropped\n",
      "FLAG_OWN_REALTY: 0 observations dropped\n",
      "REG_REGION_NOT_LIVE_REGION: 0 observations dropped\n",
      "AMT_REQ_CREDIT_BUREAU_WEEK: 282 observations dropped\n",
      "DAYS_REGISTRATION: 4749 observations dropped\n",
      "EXT_SOURCE_3: 4624 observations dropped\n",
      "DAYS_ID_PUBLISH: 4566 observations dropped\n",
      "AMT_REQ_CREDIT_BUREAU_HOUR: 1365 observations dropped\n",
      "FLAG_DOCUMENT_7: 24 observations dropped\n",
      "EXT_SOURCE_2: 4474 observations dropped\n",
      "DEF_30_CNT_SOCIAL_CIRCLE: 1070 observations dropped\n",
      "OBS_30_CNT_SOCIAL_CIRCLE: 1873 observations dropped\n",
      "FLAG_DOCUMENT_9: 743 observations dropped\n",
      "CNT_CHILDREN: 379 observations dropped\n",
      "FLAG_DOCUMENT_11: 739 observations dropped\n",
      "FLAG_DOCUMENT_17: 64 observations dropped\n",
      "FLAG_DOCUMENT_12: 0 observations dropped\n",
      "FLAG_DOCUMENT_18: 1835 observations dropped\n",
      "FLAG_PHONE: 0 observations dropped\n",
      "AMT_REQ_CREDIT_BUREAU_DAY: 793 observations dropped\n",
      "FLAG_DOCUMENT_15: 265 observations dropped\n",
      "DAYS_LAST_PHONE_CHANGE: 2111 observations dropped\n",
      "REG_CITY_NOT_LIVE_CITY: 0 observations dropped\n",
      "FLAG_DOCUMENT_8: 0 observations dropped\n",
      "FLAG_DOCUMENT_20: 69 observations dropped\n",
      "REGION_POPULATION_RELATIVE: 902 observations dropped\n",
      "REG_CITY_NOT_WORK_CITY: 0 observations dropped\n",
      "AMT_REQ_CREDIT_BUREAU_MON: 1760 observations dropped\n",
      "AMT_REQ_CREDIT_BUREAU_QRT: 1804 observations dropped\n",
      "FEMALE: 0 observations dropped\n",
      "FLAG_DOCUMENT_13: 732 observations dropped\n",
      "AMT_REQ_CREDIT_BUREAU_YEAR: 909 observations dropped\n",
      "FLAG_DOCUMENT_4: 17 observations dropped\n",
      "FLAG_DOCUMENT_16: 0 observations dropped\n",
      "FLAG_OWN_CAR: 0 observations dropped\n",
      "REG_REGION_NOT_WORK_REGION: 0 observations dropped\n",
      "AMT_GOODS_PRICE: 3301 observations dropped\n",
      "CASH_LOANS: 0 observations dropped\n",
      "DAYS_EMPLOYED: 1999 observations dropped\n",
      "FLAG_DOCUMENT_5: 0 observations dropped\n",
      "FLAG_DOCUMENT_10: 6 observations dropped\n",
      "FLAG_DOCUMENT_6: 0 observations dropped\n",
      "FLAG_EMAIL: 0 observations dropped\n",
      "Total observations dropped: 47323\n"
     ]
    }
   ],
   "source": [
    "#Trim outliers\n",
    "df_no_outs = df_no_nas.copy()\n",
    "for i in indep_vars:\n",
    "    mask = (df_no_outs[i] >= np.percentile(df_no_outs[i], 1)) & (df_no_outs[i] <= np.percentile(df_no_outs[i], 99))\n",
    "    print(f'{i}: {sum(np.invert(mask))} observations dropped')\n",
    "    df_no_outs = df_no_outs.loc[mask, :]\n",
    "print(f'Total observations dropped: {df_no_nas.shape[0] - df_no_outs.shape[0]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export both versions of the data set\n",
    "df.to_csv('D:/My stuff/School/Master/Master Thesis/Data/Final data/HomeCredit_nas.csv', index = False)\n",
    "df_no_nas.to_csv('D:/My stuff/School/Master/Master Thesis/Data/Final data/HomeCredit_main.csv', index = False)\n",
    "df_no_cats.to_csv('D:/My stuff/School/Master/Master Thesis/Data/Final data/HomeCredit_no_cats.csv', index = False)\n",
    "df_no_outs.to_csv('D:/My stuff/School/Master/Master Thesis/Data/Final data/HomeCredit_no_outs.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diploma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
